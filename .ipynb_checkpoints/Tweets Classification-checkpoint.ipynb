{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWEETS CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c358f3729b47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, string#, textblob\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'results.csv',\n",
       " 'sample_submission.csv',\n",
       " 'sentiment140_160k_tweets_train.csv',\n",
       " 'sentiment140_test.csv',\n",
       " 'Tweets Classification.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train test\n",
    "train = pd.read_csv('sentiment140_160k_tweets_train.csv')\n",
    "test = pd.read_csv('sentiment140_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>1978186076</td>\n",
       "      <td>ceruleanbreeze</td>\n",
       "      <td>@nocturnalie Anyway, and now Abby and I share ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p</td>\n",
       "      <td>1994697891</td>\n",
       "      <td>enthusiasticjen</td>\n",
       "      <td>@JoeGigantino Few times I'm trying to leave co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p</td>\n",
       "      <td>2191885992</td>\n",
       "      <td>LifeRemixed</td>\n",
       "      <td>@AngieGriffin Good Morning Angie  I'll be in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>1753662211</td>\n",
       "      <td>lovemandy</td>\n",
       "      <td>had a good day driving up mountains, visiting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p</td>\n",
       "      <td>2177442789</td>\n",
       "      <td>_LOVELYmanu</td>\n",
       "      <td>downloading some songs  i love lady GaGa.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target         ids             user  \\\n",
       "0      p  1978186076   ceruleanbreeze   \n",
       "1      p  1994697891  enthusiasticjen   \n",
       "2      p  2191885992      LifeRemixed   \n",
       "3      p  1753662211        lovemandy   \n",
       "4      p  2177442789      _LOVELYmanu   \n",
       "\n",
       "                                                text  \n",
       "0  @nocturnalie Anyway, and now Abby and I share ...  \n",
       "1  @JoeGigantino Few times I'm trying to leave co...  \n",
       "2  @AngieGriffin Good Morning Angie  I'll be in t...  \n",
       "3  had a good day driving up mountains, visiting ...  \n",
       "4          downloading some songs  i love lady GaGa.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1966755320</td>\n",
       "      <td>dbuie</td>\n",
       "      <td>my twitter is dead where did everybody go  @T_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1676776141</td>\n",
       "      <td>MissTierra</td>\n",
       "      <td>@theasiangoddess OUCH! that really looks like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2052521105</td>\n",
       "      <td>CaliBelle</td>\n",
       "      <td>@3fingaz I cant read it frm ma phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956499203</td>\n",
       "      <td>extraorDANaire1</td>\n",
       "      <td>Never go clubbing with a couple...you'll only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1880896910</td>\n",
       "      <td>dinnie</td>\n",
       "      <td>Not to advertise. But www.funzsquare.com is my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ids             user  \\\n",
       "0  1966755320            dbuie   \n",
       "1  1676776141       MissTierra   \n",
       "2  2052521105        CaliBelle   \n",
       "3  1956499203  extraorDANaire1   \n",
       "4  1880896910           dinnie   \n",
       "\n",
       "                                                text  \n",
       "0  my twitter is dead where did everybody go  @T_...  \n",
       "1  @theasiangoddess OUCH! that really looks like ...  \n",
       "2              @3fingaz I cant read it frm ma phone   \n",
       "3  Never go clubbing with a couple...you'll only ...  \n",
       "4  Not to advertise. But www.funzsquare.com is my...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lost_dog</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcraddictal</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webwoke</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetpet</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SallytheShizzle</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enamoredsoul</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DarkPiano</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VioletsCRUK</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what_bugs_u</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayme1988</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michxxblc</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaybranch</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dogbook</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StDAY</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_magic8ball</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text\n",
       "user                 \n",
       "lost_dog           55\n",
       "mcraddictal        33\n",
       "webwoke            33\n",
       "tweetpet           33\n",
       "SallytheShizzle    29\n",
       "enamoredsoul       28\n",
       "DarkPiano          27\n",
       "VioletsCRUK        27\n",
       "what_bugs_u        26\n",
       "Jayme1988          25\n",
       "michxxblc          24\n",
       "jaybranch          23\n",
       "Dogbook            23\n",
       "StDAY              23\n",
       "_magic8ball        23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb of tweet per user\n",
    "train[['text','user']].groupby(['user']).count().sort_values('text',ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    123402\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['text','user']].groupby(['user']).count().sort_values('text',ascending = False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159985"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['p', 'n'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24e82a26f60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "targetMAP = {'p':1, 'n':0}\n",
    "train['target_bool'] = train['target'].map(targetMAP)\n",
    "train = train.drop('target',1)\n",
    "train['target_bool'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>target_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1978186076</td>\n",
       "      <td>ceruleanbreeze</td>\n",
       "      <td>@nocturnalie Anyway, and now Abby and I share ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1994697891</td>\n",
       "      <td>enthusiasticjen</td>\n",
       "      <td>@JoeGigantino Few times I'm trying to leave co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2191885992</td>\n",
       "      <td>LifeRemixed</td>\n",
       "      <td>@AngieGriffin Good Morning Angie  I'll be in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1753662211</td>\n",
       "      <td>lovemandy</td>\n",
       "      <td>had a good day driving up mountains, visiting ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2177442789</td>\n",
       "      <td>_LOVELYmanu</td>\n",
       "      <td>downloading some songs  i love lady GaGa.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ids             user  \\\n",
       "0  1978186076   ceruleanbreeze   \n",
       "1  1994697891  enthusiasticjen   \n",
       "2  2191885992      LifeRemixed   \n",
       "3  1753662211        lovemandy   \n",
       "4  2177442789      _LOVELYmanu   \n",
       "\n",
       "                                                text  target_bool  \n",
       "0  @nocturnalie Anyway, and now Abby and I share ...            1  \n",
       "1  @JoeGigantino Few times I'm trying to leave co...            1  \n",
       "2  @AngieGriffin Good Morning Angie  I'll be in t...            1  \n",
       "3  had a good day driving up mountains, visiting ...            1  \n",
       "4          downloading some songs  i love lady GaGa.            1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to clean the text, get rid of useless words and tipos\n",
    "\n",
    "https://www.pythonforbeginners.com/regex/regular-expressions-in-python\n",
    "\n",
    "https://python-django.dev/page-expressions-regulieres-regular-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mentions : if a specific mention contains only positive tweets the feature might be added in the model...\n",
    "text = re.sub(r'@\\w+', '', train['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@nocturnalie Anyway, and now Abby and I share all our crops. We have a very healthy Master/Minion relationship \n",
      " Anyway, and now Abby and I share all our crops. We have a very healthy Master/Minion relationship \n"
     ]
    }
   ],
   "source": [
    "print(train['text'][0])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2 = 'hello visit the website: https://youyou.fr'\n",
    "\n",
    "# urls : do not bring predictive power to the model\n",
    "text2 = re.sub(r'http.?://[^/s]+[/s]?', '', ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello visit the website: https://youyou.fr\n",
      "hello visit the website: \n"
     ]
    }
   ],
   "source": [
    "print(ex2)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3 = 'i like1 2 3 #wow'\n",
    "\n",
    "## let's remove symbols and digits as well\n",
    "text3 = re.sub('[^a-zA-Z\\s]', '', ex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like1 2 3 #wow\n",
      "i like   wow\n"
     ]
    }
   ],
   "source": [
    "print(ex3)\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex4 = '   wow il  fait beau   '\n",
    "\n",
    "# remove extra spaces\n",
    "text4 = re.sub(\"\\s+\", '', ex4)\n",
    "# remove leading spaces\n",
    "text5 = ex4.lstrip()\n",
    "# remove trailing spaces\n",
    "text6 = ex4.rstrip()\n",
    "\n",
    "text7 = text4.lstrip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   wow il  fait beau   \n",
      "remove extra spaces   : [wowilfaitbeau]\n",
      "remove leading spaces : [wow il  fait beau   ]\n",
      "remove trailing spaces: [   wow il  fait beau]\n",
      "remove all spaces     : [wowilfaitbeau]\n"
     ]
    }
   ],
   "source": [
    "print(ex4)\n",
    "print(f'remove extra spaces   : [{text4}]')\n",
    "print(f'remove leading spaces : [{text5}]')\n",
    "print(f'remove trailing spaces: [{text6}]')\n",
    "print(f'remove all spaces     : [{text7}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex5 = 'EdLo L Ik kO LLL'\n",
    "\n",
    "# only lower char\n",
    "text8 = ex5.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EdLo L Ik kO LLL\n",
      "edlo l ik ko lll\n"
     ]
    }
   ],
   "source": [
    "print(ex5)\n",
    "print(text8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, using a spell checker to correct mistakes\n",
    "\n",
    "https://norvig.com/spell-correct.html?source=post_page---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http.?://[^/s]+[/s]?', '', text)\n",
    "    text = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_mentions(dataset, feature):\n",
    "#     nb_obs = dataset.shape[0]\n",
    "#     new_text = [re.sub(r'@\\w+', '', dataset.loc[i, feature]) for i in range(nb_obs)]\n",
    "#     dataset[feature] = new_text\n",
    "#     return(dataset)\n",
    "\n",
    "# def remove_urls(dataset, feature):\n",
    "#     nb_obs = dataset.shape[0]\n",
    "#     new_text = [re.sub(r'http.?://[^/s]+[/s]?', '', dataset.loc[i, feature]) for i in range(nb_obs)]\n",
    "#     dataset[feature] = new_text\n",
    "#     return(dataset)\n",
    "\n",
    "# def remove_symbols(dataset, feature):\n",
    "#     nb_obs = dataset.shape[0]\n",
    "#     new_text = [re.sub('[^a-zA-Z\\s]', '', dataset.loc[i, feature]) for i in range(nb_obs)]\n",
    "#     dataset[feature] = new_text\n",
    "#     return(dataset)\n",
    "\n",
    "# def to_lower(dataset, feature):\n",
    "#     nb_obs = dataset.shape[0]\n",
    "#     new_text = [dataset.loc[i, feature].lower() for i in range(nb_obs)]\n",
    "#     dataset[feature] = new_text\n",
    "#     return(dataset)\n",
    "\n",
    "# def clean_text(dataset, feature):\n",
    "#     dataset = remove_mentions(dataset, feature)\n",
    "#     dataset = remove_urls(dataset, feature)\n",
    "#     dataset = remove_symbols(dataset, feature)\n",
    "#     dataset = to_lower(dataset, feature)\n",
    "#     return(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_text(train, 'text')\n",
    "\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n",
      "\n",
      "{'if', 'have', 'which', 'five', 'been', 'those', 'somehow', 'thru', 'cannot', 'yours', 'that', 'she', 'never', 'besides', 'forty', '’ll', 'made', 'against', 'where', 'out', 'some', 'every', 'three', 'in', 'when', 'whose', 'us', 'hundred', 'wherever', 'six', 'everywhere', 'as', 'many', 'everything', 'do', '’m', 'beforehand', 'from', 'bottom', 'anyone', 'our', 'please', 'this', 'now', \"'ve\", 'he', 'due', 'such', 'perhaps', 'part', 'whom', 'may', 'less', 'back', 'everyone', 'neither', 'themselves', 'twenty', '‘re', 'whether', 'though', 'its', 'why', 'but', 'their', 'hereupon', 'whence', 'not', 'once', 'sometimes', 'should', 'what', 'between', 'these', 'often', '‘ve', 'done', 'well', 'along', 'upon', 'fifty', 'latter', 'it', 'doing', 'empty', 'two', 'above', 'someone', 'here', 'beside', 'nor', 'down', 'so', 'seeming', 'myself', 'each', 'them', 'must', 'hers', 'by', 'your', 'else', \"'ll\", \"'re\", \"'s\", 'already', 'call', 'towards', 'still', 'seems', 'twelve', 'noone', 'latterly', 'much', 'former', 'go', 'others', 'meanwhile', 'own', 'were', '‘ll', 'hereafter', 'eight', 'via', 'anyhow', '‘m', 'her', 'of', 'or', 'became', 'nevertheless', 'amongst', 'becoming', 'be', 'namely', 'somewhere', 'unless', 'more', 'per', 'until', '’re', 'any', 'only', 'almost', 'several', 'most', 'thereafter', 'full', 'top', 'whereas', 'sometime', 'say', 'ca', 'ours', 'serious', 'yourself', 'all', 'show', 'whatever', 'none', 'first', 'with', 'his', '‘d', 'can', 'one', 'give', 'while', 'whereupon', 'on', 'together', 'last', 'does', 'a', 'might', 'becomes', 'few', 'me', 'enough', 'whereby', 'very', 'moreover', 'really', 'get', 'up', 'front', 'toward', 'anyway', 'would', 'even', 'mostly', 'always', 'rather', 'make', 'himself', 'than', 'who', \"'m\", 'both', 'thereupon', 'during', 'become', 'no', 'being', '‘s', 'across', 'however', 'thus', 'whither', 'next', 'itself', 'therefore', 'through', 'something', 'whole', 'seem', 'n’t', 'thereby', 'too', 'whenever', 'throughout', 'nine', 'has', 'four', 'formerly', 'also', 'quite', 'within', 'behind', 'keep', 'without', 'nothing', 'move', 'name', 'just', 'same', 'third', 'anything', 'sixty', 'off', 'using', 'herself', 'the', 'ourselves', 'various', 'had', 'to', 'yourselves', 'therein', 'anywhere', 'indeed', 'seemed', 'over', 'we', 'and', 'herein', 'although', 'my', 'alone', 'elsewhere', 'another', 'except', \"'d\", 'could', 'amount', 'thence', 'hereby', \"n't\", 'they', 'because', 'least', 'again', 'ten', 'around', 'nowhere', 'him', 'otherwise', 'beyond', 'into', 'an', 'under', 'further', 'you', 'either', 'was', 'how', 'since', 'there', 'at', 'before', 'hence', 'i', '’ve', 'ever', 'whoever', 'put', 'after', 'see', 'whereafter', 'n‘t', 'did', 'afterwards', 'are', 're', 'nobody', 'am', 'then', 'for', 'used', 'side', 'onto', 'yet', '’s', 'about', 'other', 'wherein', 'among', 'fifteen', 'will', '’d', 'mine', 'below', 'is', 'take', 'eleven', 'regarding'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run in shell : python -m spacy download en\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "print(punctuations, '\\n')\n",
    "\n",
    "# Create a list of stopwords\n",
    "# nlp = spacy.load('en')\n",
    "# print(nlp, '\\n')\n",
    "\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(stop_words, '\\n')\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Create a tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Create a token object, used to create documents with linguistic annotations\n",
    "    mytokens = parser(sentence)\n",
    "    \n",
    "    # Lemmatizing each token\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != '-PRON-' else word.lower_ for word in mytokens]\n",
    "    \n",
    "    # Removing stopwords and punctuations\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations]\n",
    "    \n",
    "    return(mytokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@joegigantino',\n",
       " 'time',\n",
       " 'try',\n",
       " 'leave',\n",
       " 'comment',\n",
       " 'blog',\n",
       " 'message',\n",
       " 'quot;operation',\n",
       " 'aborted&quot',\n",
       " 'clue']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer(train.loc[1,'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying text in positive and negative labels is called sentiment analysis. So we need a way to represent our text numerically.\n",
    "\n",
    "One tool we can use for doing this is called Bag of Words. BoW converts text into the matrix of occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 5), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function spacy_tokenizer at 0x0000024E84E18C80>,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "# ngram_range is lower and upper range of the context\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,5))\n",
    "print(bow_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Term Frequency-Inverse Document Frequency$$\n",
    "\n",
    "\n",
    "$$ idf(W) = \\frac{nb\\_documents}{nb\\_documents\\_containing\\_word\\_W} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a way of normalizing our Bag of Words(BoW) by looking at each word’s frequency in comparison to the document frequency. In other words, it’s a way of representing how important a particular term is in the context of a given document, based on how many times the term appears and how many other documents that same term appears in. The higher the TF-IDF, the more important that term is to that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function spacy_tokenizer at 0x0000024E84E18C80>,\n",
      "        use_idf=True, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "print(tfidf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_v, X_test_v, y_train_v, y_test_v = train_test_split(train['text'], train['target_bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector.fit(train['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "X_train_v_count =  bow_vector.transform(X_train_v)\n",
    "X_test_v_count =  bow_vector.transform(X_test_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting The Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train['text']\n",
    "# y_train = train['target_bool']\n",
    "\n",
    "# X_test = test['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline and Generating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return(metrics.accuracy_score(predictions, y_test_v),metrics.precision_score(predictions, y_test_v),metrics.recall_score(predictions, y_test_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector.fit(train['text'])\n",
    "xtrain_tfidf =  tfidf_vector.transform(X_train_v)\n",
    "xvalid_tfidf =  tfidf_vector.transform(X_test_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naive Bayes: 0.7338300372527939\n",
      "Precision Naive Bayes: 0.6842816393934844\n",
      "Recall Naive Bayes: 0.7592027094553329\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "results_nb = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, y_train_v, xvalid_tfidf)\n",
    "print(f'Accuracy Naive Bayes: {results_nb[0]}\\nPrecision Naive Bayes: {results_nb[1]}\\nRecall Naive Bayes: {results_nb[2]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x0000024E9CBC0FD0>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "      ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create a pipeline using BoW\n",
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Model generation\n",
    "pipe.fit(X_train_v, y_train_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model Accuracy on train\n",
    "# print(\"Logistic Regression Accuracy LR:\",metrics.accuracy_score(y_test_v, predicted))\n",
    "# print(\"Logistic Regression Precision LR:\",metrics.precision_score(y_test_v, predicted))\n",
    "# print(\"Logistic Regression Recall LR:\",metrics.recall_score(y_test_v, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Logistic Regression: 0.7519813986048953\n",
      "Precision Logistic Regression: 0.7388559221457394\n",
      "Recall Logistic Regression: 0.7788620327278186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LR on Ngram Level TF IDF Vectors\n",
    "print(f\"Accuracy Logistic Regression: {metrics.accuracy_score(y_test_v, predicted)}\\nPrecision Logistic Regression: {metrics.precision_score(y_test_v, predicted)}\\nRecall Logistic Regression: {metrics.recall_score(y_test_v, predicted)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "results_rf = train_model(ensemble.RandomForestClassifier(), X_train_v_count, y_train_v, X_test_v_count)\n",
    "print(f\"Accuracy RF: {results_rf[0]}\\nPrecision RF: {results_rf[1]}\\nRecall RF: {results_rf[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"Xgb, Count Vectors: \", accuracy)? (<ipython-input-38-6bc163ac2454>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-6bc163ac2454>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    print \"Xgb, Count Vectors: \", accuracy\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"Xgb, Count Vectors: \", accuracy)?\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "results_xgb = train_model(xgb.XGBClassifier(), X_train_v_count.tocsc(), y_train_v, X_test_v_count.tocsc())\n",
    "print(f\"Accuracy XGB: {results_xgb[0]}\\nPrecision XGB: {results_xgb[1]}\\nRecall XGB: {results_xgb[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer = optimizers.Adam(), loss = 'binary_crossentropy')\n",
    "    return(classifier)\n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf.shape[1])\n",
    "results_nn = train_model(classifier, xtrain_tfidf, y_train_v, xvalid_tfidf, is_neural_net=True)\n",
    "print(f\"Accuracy NN: {results_nn[0]}\\nPrecision NN: {results_nn[1]}\\nRecall NN: {results_n[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "results_cnn = train_model(classifier, train_seq_x, y_train_v, valid_seq_x, is_neural_net=True)\n",
    "print(f\"Accuracy CNN: {results_nn[0]}\\nPrecision CNN: {results_nn[1]}\\nRecall CNN: {results_n[2]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "results_lstm = train_model(classifier, train_seq_x, y_train_v, valid_seq_x, is_neural_net=True)\n",
    "print(f\"Accuracy LSTM: {results_lstm[0]}\\nPrecision CNN: {results_lstm[1]}\\nRecall CNN: {results_lstm[2]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result dataframe\n",
    "d = {'target': predicted, 'ids': test['ids']}\n",
    "results = pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_reMAP = {1:'p', 0:'n'}\n",
    "results['target'] = results['target'].map(predict_reMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file\n",
    "results.to_csv('results.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
